package com.lisz.bigdata.spark

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object Lesson03_rdd_aggregator {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setMaster("local").setAppName("test")
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR")

    val data = sc.parallelize(List(
      ("zhang san", 234),
      ("zhang san", 5567),
      ("zhang san", 343),
      ("li si", 212),
      ("li si", 44),
      ("li si", 33),
      ("wang wu", 535),
      ("wang wu", 22)
    ))
    // data is Key-Value pairs
    val group = data.groupByKey()
    group.foreach(println)
    // è¡Œåˆ—è½¬æ¢:zhang sançš„ä¸‰è¡Œç»è¿‡groupByKeyä¹‹åŽè½¬æˆäº†valueä¸­çš„ä¸‰åˆ—, ä¸‹é¢å†è½¬æ¢å›žæ¥ï¼š
    group.flatMap(e=>e._2.map((e._1,_))).foreach(println)
    println("--------------------------")

    // ä¸‹é¢çš„ä¹Ÿè¡Œ
    group.flatMap(e => e._2.map(x => (e._1, x)).iterator).foreach(println)
    println("--------------------------")

    // ä¸‹é¢çš„ä¹Ÿè¡Œ, è·ŸflatMapä¸€æ ·ï¼Œæ˜¯è¿›ä¸€æ¡å‡ºåŽ»å¤šæ¡ scalaä¼šå¸®æˆ‘ä»¬æŠŠkeyæ‹¼ä¸Šå„ä¸ªvalueé‡Œé¢çš„æ•°å­—ï¼Œå¯¹valueåšèšåˆï¼Œkeyå”¯ä¸€. e åªæ˜¯Tuple2ä¸­çš„value, ä½†è¿™ä¸ªvallueåˆå¯ä»¥è¿­ä»£
    group.flatMapValues(e => e.iterator).foreach(println)
    println("--------------------------")

    //group.mapValues(e => e.toList.sorted.take(2)).flatMapValues(e => e.iterator).foreach(println)
    // ä¸‹é¢çš„ä¹Ÿè¡Œï¼ŒflatMapValuesæ˜¯åˆ—è½¬è¡Œï¼šä¸€æ¡ key - list å˜æˆå¤šä¸ª key - value
    group.flatMapValues(_.toList.sorted.take(2)).foreach(println)
    println("--------------------------")

    println("------- sum, count, min, max avg -------------------")
    val sum = data.reduceByKey(_ + _)
    val max = data.reduceByKey((oldValue, newValue) => if (oldValue > newValue) oldValue else newValue)
    val min = data.reduceByKey((oldValue, newValue) => if (oldValue < newValue) oldValue else newValue)
    val count1 = data.map(x => (x._1, 1)).reduceByKey(_+_)
    val count2 = data.mapValues(e => 1).reduceByKey(_ + _)


    println("--------- sum ----------------")
    sum.foreach(println)

    println("--------- max ----------------")
    max.foreach(println)

    println("--------- min ----------------")
    min.foreach(println)

    println("--------- count1 ----------------")
    count1.foreach(println)
    println("--------- count2 ----------------")
    count2.foreach(println)

    println("--------- avg1 -----------------")
    // ðŸ‘‡è‡ªå·±å†™å‡ºæ¥çš„ avg ^_^
    data.mapValues(x=>(x,1)).reduceByKey((oldVal, newVal)=>(oldVal._1 + newVal._1, oldVal._2 + newVal._2)).mapValues(x=>(x._1 * 1.0 / x._2)).foreach(println)
    println("--------- avg2 -----------------")
    /*
      (zhang san,(6144, 3))
      (wang wu,(557, 2))
      (li si,(289,3))
     */
    // Sean çš„æ–¹æ³•ï¼Œåˆ©ç”¨äº†ä¹‹å‰çš„ç»“æžœå’Œjoinæ“ä½œ
    val tmp: RDD[(String, (Int, Int))] = sum.join(count2)
    tmp.mapValues(x=>(x._1 * 1.0 / x._2)).foreach(println)

    println("--------- avg3 combiner --------")
    val tmpx = data.combineByKey(
      // createCombiner: V => C, ç¬¬ä¸€æ¡è®°å½•çš„ value æ€Žä¹ˆæ”¾å…¥ hashmap
      (value: Int) => (value, 1),
      // mergeValue: (C, V) => C,å¦‚æžœæœ‰ç¬¬äºŒæ¡è®°å½•ï¼Œç¬¬äºŒæ¡åŠä»¥åŽçš„ä»–ä»¬çš„valueæ€Žä¹ˆæ”¾åˆ°hashMapé‡Œ
      (oldValue: (Int, Int), newValue: Int) => (oldValue._1 + newValue, oldValue._2 + 1),
      // mergeCombiners: (C, C) => C åˆå¹¶æº¢å†™ç»“æžœçš„å‡½æ•°
      (oldValue: (Int, Int), newValue: (Int, Int)) => (oldValue._1 + newValue._1, oldValue._2 + newValue._2)
    )
    tmpx.mapValues(e => e._1 * 1.0 / e._2).foreach(println)

    Thread.sleep(Long.MaxValue)
  }

}
