package com.lisz.bigdata2.spark

import org.apache.spark.{SparkConf, SparkContext}

/**
 * RDDåŠå…¶åº”ç”¨å®Œç»“ç¯‡ï¼Œä¸‹é¢è½¬å…¥æºç åˆ†æï¼Œç²¾å½©ç»§ç»­ğŸ˜„
 */
object Lesson06_rdd_over_2 {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setMaster("local").setAppName("topN")
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR")

    val data = sc.parallelize(List(
      "hello world",
      "hello spark",
      "hello world",
      "hello world",
      "hello hadoop",
      "hello world",
      "hello lisz"
    ))
    val res = data.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _)
    // æ¯”ä¸Šé¢çš„å†™æ³•å°‘ä¸€æ¬¡shuffleï¼keyæ²¡æœ‰å‘ç”Ÿå˜åŒ–ã€åˆ†åŒºå™¨æ²¡æœ‰å‘ç”Ÿå˜åŒ–ã€åˆ†åŒºæ•°æ²¡æœ‰å‘ç”Ÿå˜åŒ–ã€‚å»ºè®®ç”¨mapValueså’ŒflatMapValuesï¼Œ
    // å› ä¸ºä»–ä»¬ä¼šä¼ preservesPartitioning = trueä¸ä¸¢å¼ƒåˆ†åŒºå™¨ï¼Œä¸ç”¨shuffle
    // mapä¼šä¸¢å¼ƒå‰é¢çš„åˆ†åŒºï¼Œå¯¼è‡´å¤šä½™çš„shuffle
    val res01 = res.mapValues(_ * 10)
    val res02 = res01.groupByKey()
    res02.foreach(println)

    Thread.sleep(1000000)
  }
}
